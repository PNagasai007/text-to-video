# -*- coding: utf-8 -*-
"""FFINALLLAMA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F6uqFk6spVkAVpjO59jr0jCjeWA26HVI
"""

!pip install -q transformers einops accelerate langchain bitsandbytes

!huggingface-cli login --token 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





!pip install sentencepiece

from langchain import HuggingFacePipeline
from transformers import AutoTokenizer
import transformers
import torch

model = "meta-llama/Llama-2-7b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model)

pipeline = transformers.pipeline(
    "text-generation", #task
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
    max_length=1000,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id
)

llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})

from langchain import PromptTemplate,  LLMChain

template = """
              Write a concise summary of the following text delimited by triple backquotes.
              Return your response in a string which covers the key points of the text in .
              ```{text}```
              BULLET POINT SUMMARY:
           """

prompt = PromptTemplate(template=template, input_variables=["text"])

llm_chain = LLMChain(prompt=prompt, llm=llm)


# print(llm_chain.run(text))

text = """
Toyota and Lexus are recalling nearly 100,000 vehicles in Canada due to the possibility the airbag may not deploy.

In a statement Wednesday, Toyota said it is conducting a safety recall for about 99,965 of its 2020-2022 model year Toyota and Lexus vehicles.

The recall is due to sensors in the front passenger seat that may have been “improperly manufactured, causing a short circuit,” the company said.

“This would not allow the airbag system to properly classify the occupant’s weight, and the airbag may not deploy as designed in certain crashes, increasing the risk of injury.”
"""

a=llm_chain.run(text)

print(type(a))
print(a)
# l=a.split('•')
# print(l)

text_list = [line.strip() for line in a.split('\n')]

print(len(text_list))
print((text_list))
l=text_list
for i in range(len(l)):
  l[i]=l[i][:-1]
  l[i]=l[i][1:]
filtered_list = list(filter(None,l))

print(filtered_list)
y=filtered_list
print(y)

# # l=a.split('            ')
# print(l)
# l[0]=l[0][1:]

# for i in range(len(l)):
#   l[i]=l[i][:-1]
#   l[i]=l[i][1:]

# print(l)
# y=l
# print(y)
# print(len(y))