# -*- coding: utf-8 -*-
"""Finaloutput.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iFQFiVtLOjkyM1FmnGlsRaO0zU4JatUA
"""

#installations and login HF for llama
!pip install -q transformers einops accelerate langchain bitsandbytes
!huggingface-cli login --token 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
!pip install sentencepiece

#installations for image generation (sdxl) and video manipulation

!pip install invisible_watermark transformers accelerate safetensors
!pip install diffusers
from diffusers import DiffusionPipeline
import torch
!pip install accelerate
!pip install matplotlib
!pip install Image pytesseract
!pip install Image
from PIL import Image
!pip install pydub
! pip install moviepy
!pip install gTTS

#loading model llama
from langchain import HuggingFacePipeline
from transformers import AutoTokenizer
import transformers
import torch

model = "meta-llama/Llama-2-7b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model)

pipeline = transformers.pipeline(
    "text-generation", #task
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
    max_length=1000,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id
)
llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})
from langchain import PromptTemplate,  LLMChain
#giving template for getting summry
template = """
              Write a concise summary of the following text delimited by triple backquotes.
              Return your response in a string which covers the key points of the text in .
              ```{text}```
              BULLET POINT SUMMARY:
           """

prompt = PromptTemplate(template=template, input_variables=["text"])

llm_chain = LLMChain(prompt=prompt, llm=llm)


# print(llm_chain.run(text))

text = """
KL Rahul the new boss in Virat Kohli's yard as India break free of familiar old rut in World Cup opener.Shortly after Rohit Sharma took over as India's ODI captain in November of 2021, he straightaway addressed the elephant in the room. Rohit being Rohit, did not beat around the bush. Unlike his predecessor Virat Kohli, whose '45 minutes of bad cricket' analogy received mixed reactions after India were knocked out in the semifinal of the 2019 World Cup, Rohit cutting straight to the point was a breath of fresh air. "I want the middle order to be ready for 10/3 situation," he had said. Succeeding Kohli as India captain wasn't going to be a walk in the park, and for a substantial duration, Rohit confronted the challenging reality as India grappled with setbacks, failing to reach last year's Asia Cup final and enduring a shock series defeat against Bangladesh. But almost two years later, things seem to be falling in place… brick by brick.On Sunday, just two overs into the Indian innings, haunting memories of Old Trafford, Champions Trophy 2017 and the T20 World Cup 2021 came surging back. Ishan Kishan, Rohit Sharma, and Shreyas Iyer, all were dismissed for ducks. This was even more disheartening than the CT '17 and WC '19, where the top order had at least managed to get off the mark. At 2/3, the 200-run target looked as big as 270-280. In a surreal coincidence, the giant screen displayed 5/3 in 3.1 overs, exactly mirroring India's dire situation four years earlier in Manchester against New Zealand. We all remember what unfolded back then – the team's failure to recover from the early loss of wickets cost India the World Cup, a theme that set a gloomy precedent.
"""

a=llm_chain.run(text)

text_list = [line.strip() for line in a.split('\n')]

print(len(text_list))
print((text_list))
l=text_list
for i in range(len(l)):
  l[i]=l[i][:-1]
  l[i]=l[i][1:]

y=l
print(y)
# y is the list of prompts

#loading sdxl for generation of images
pipe = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16"
)
pipe.to("cuda")

# iteratively calling function to generate images



images = []
def generate_image(prompt):

    image = pipe(prompt=prompt).images[0]
    return image


for prompt in y:
    for j in range(2):
      generated_image = generate_image(prompt)
      images.append(generated_image)

#list of length of audios
from pydub import AudioSegment
from gtts import gTTS
import time
import random
import math
la=[]
aud=[]
def aa(g):

  timestamp = int(time.time())  # Get the current timestamp
  random_number = random.randint(1, 1000)  # Generate a random number


  text = g
  min_duration = 10
  min_characters = int(min_duration * 1000)
  text = text[:min_characters]
  tts = gTTS(text)
  tts.save(f'output_audio_{timestamp}_{random_number}.mp3')
  output_audio_file = f'output_audio_{timestamp}_{random_number}.mp3'
  audio = AudioSegment.from_mp3(output_audio_file)
  audio_duration_ms = len(audio)
  a=audio_duration_ms / 1000
  aud.append(output_audio_file)
  return a

for i in y:
  x=aa(i)
  la.append(x)
print(la)

print(images)
import os
os.mkdir( os.path.join("/content", "imag"))
import io
import PIL.Image as Image
import os


for i in range(len(images)):
  buffer = io.BytesIO()
  images[i].save(buffer, format='PNG')
  image_bytes = buffer.getvalue()
  with open(f'/content/imag/image{i}.png', 'wb') as f:
    f.write(image_bytes)
imags=[]
for i in range(14):

  imags.append(f"/content/imag/image{i}.png")

import time
import random
import moviepy.editor as mp
from moviepy.editor import ImageClip, concatenate_videoclips
import numpy as np
clips = []

def create_zooming_video(image_path, output_path, duration, resolution=(720,480)):
    img = mp.ImageClip(image_path)

    # Calculate the initial and final zoom scales
    initial_zoom = 1.5
    final_zoom = 1.7

    # Calculate the zooming scale for each frame
    zoom_scales = [initial_zoom + (final_zoom - initial_zoom) * t / (duration * 30) for t in range(int(duration * 30))]

    # Calculate the dimensions for the cropped image
    cropped_width = resolution[0] / final_zoom
    cropped_height = resolution[1] / final_zoom

    # Resize the image to fit the cropped dimensions
    img = img.resize((int(cropped_width), int(cropped_height)))

    # Create a list to hold each frame of the zooming animation
    zooming_frames = []

    for scale in zoom_scales:
        frame = img.resize(scale)
        frame = frame.set_position(("center", "center"))
        frame = frame.set_duration(1 / 30)  # Each frame lasts 1/30 seconds
        zooming_frames.append(frame)

    # Create the zooming animation clip
    zooming_clip = concatenate_videoclips(zooming_frames, method="compose")

    # Calculate position to keep the zoomed image centered
    position = ("center", "center")
    zooming_clip = zooming_clip.set_position(position)
    zooming_clip = zooming_clip.set_duration(duration)
    zooming_clip = zooming_clip.set_fps(30)
    zooming_clip.write_videofile(output_path, codec="libx264")
    clips.append(zooming_clip)

# Assuming you have defined images and la
k=0;
for i in range(len(la)):
  for j in range(k,k+2):
    timestamp = int(time.time())  # Get the current timestamp
    random_number = random.randint(1, 10)  # Generate a random number
    output_video = f'output_video_{timestamp}_{random_number}.mp4'
    create_zooming_video(imags[j], output_video, la[i]/2)
  k=j+1

# Concatenate the video clips
video_clip = concatenate_videoclips(clips, method='compose')

# Write the video to a file
video_clip.write_videofile("video-output.mp4", fps=24, remove_temp=True, codec="libx264", audio_codec="aac")

#final video length
from moviepy.editor import VideoFileClip

# Load the video clip
video_clip = VideoFileClip("video-output.mp4")

# Get the duration of the video in seconds
video_duration_seconds = video_clip.duration

print(f"Video duration: {video_duration_seconds} seconds")
vl=int(video_duration_seconds)

from pydub import AudioSegment #concatenate aud

# Create an empty AudioSegment to store the concatenated audio
combined_audio = AudioSegment.empty()

# List of audio file paths to concatenate


# Iterate through the list and concatenate the audio files
for audio_file in aud:
    audio_segment = AudioSegment.from_file(audio_file)
    combined_audio += audio_segment

# Export the concatenated audio
combined_audio.export("output.mp3", format="mp3")
print(len(combined_audio))

#concatenation of viedo and audio
import cv2
import numpy as np
from PIL import Image
from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip, AudioFileClip
from moviepy.audio.fx import audio_fadein
video_clip = VideoFileClip("video-output.mp4")
audio_clip = AudioFileClip("output.mp3")
audio_clip = audio_clip.subclip(0, video_clip.duration)
video_clip = video_clip.set_audio(audio_clip)
output_video_with_audio = "ffinal.mp4"
video_clip.write_videofile(output_video_with_audio, codec='libx264', fps=24)

